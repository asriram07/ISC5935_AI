{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "8_Ymjm8SMa-U",
        "outputId": "3377ed3e-d051-4002-8ad1-4bb930d5ab38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.12\n",
            "  Downloading torchtext-0.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12) (2.32.3)\n",
            "Collecting torch==1.11.0 (from torchtext==0.12)\n",
            "  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.12) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->torchtext==0.12) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.12) (2024.8.30)\n",
            "Downloading torchtext-0.12.0-cp310-cp310-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl (750.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu121\n",
            "    Uninstalling torch-2.5.1+cu121:\n",
            "      Successfully uninstalled torch-2.5.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.13.2 requires torch>=1.13.0, but you have torch 1.11.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.11.0 which is incompatible.\n",
            "torchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 1.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0 torchtext-0.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch"
                ]
              },
              "id": "74b267b5ba57419697e199b04e2e43e1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "sxoIvUaTMO_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import spacy\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "# General settings\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 0.005\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "NUM_CLASSES = 2"
      ],
      "metadata": {
        "id": "NW3YT7LfMZbf"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('movie_data.csv')\n",
        "df.columns = ['TEXT_COLUMN_NAME', 'LABEL_COLUMN_NAME']\n",
        "df.to_csv('movie_data.csv', index=None)\n",
        "\n",
        "# Tokenizer and Vocabulary\n",
        "tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxe1GwGpMZeF",
        "outputId": "0fdca276-72e5-4ac8-9f72-335a99f53d63"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(texts, max_size):\n",
        "    token_freqs = {}\n",
        "    for text in texts:\n",
        "        tokens = tokenizer(text)\n",
        "        for token in tokens:\n",
        "            token_freqs[token] = token_freqs.get(token, 0) + 1\n",
        "    sorted_tokens = sorted(token_freqs.items(), key=lambda x: x[1], reverse=True)\n",
        "    vocab = {word: idx + 2 for idx, (word, _) in enumerate(sorted_tokens[:max_size])}\n",
        "    vocab['<unk>'] = 0\n",
        "    vocab['<pad>'] = 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "OsbU9GmlMZg2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    df['TEXT_COLUMN_NAME'], df['LABEL_COLUMN_NAME'], test_size=0.2, random_state=RANDOM_SEED\n",
        ")\n",
        "train_texts, valid_texts, train_labels, valid_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.15, random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "vocab = build_vocab(train_texts.tolist(), VOCABULARY_SIZE)"
      ],
      "metadata": {
        "id": "LEDRWNlJNA1j"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_text(text):\n",
        "    tokens = tokenizer(text)\n",
        "    return [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "\n",
        "def encode_label(label):\n",
        "    return int(label)"
      ],
      "metadata": {
        "id": "h3tz9N3JNLfg"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = encode_text(self.texts[idx])\n",
        "        label = encode_label(self.labels[idx])\n",
        "        return torch.tensor(text, dtype=torch.long), torch.tensor(label, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "q_G9VghdNLiU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(train_texts.tolist(), train_labels.tolist())\n",
        "valid_dataset = TextDataset(valid_texts.tolist(), valid_labels.tolist())\n",
        "test_dataset = TextDataset(test_texts.tolist(), test_labels.tolist())"
      ],
      "metadata": {
        "id": "LYqy7ksNNfEA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    lengths = torch.tensor([len(text) for text in texts])\n",
        "    texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=vocab['<pad>'])\n",
        "    labels = torch.tensor(labels)\n",
        "    return texts, lengths, labels\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "VY6LgPezNA3_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(input_dim, embedding_dim, padding_idx=vocab['<pad>'])\n",
        "        self.rnn = torch.nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        embedded = self.embedding(text)\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        output = self.fc(hidden[-1])\n",
        "        return output"
      ],
      "metadata": {
        "id": "XZEnsehVNA6V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN(input_dim=len(vocab), embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, output_dim=NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "GA4I5meJNA80"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(model, data_loader):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for features, lengths, labels in data_loader:\n",
        "            features, lengths, labels = features.to(DEVICE), lengths.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(features, lengths)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            correct_pred += (predictions == labels).sum().item()\n",
        "            num_examples += labels.size(0)\n",
        "    return correct_pred / num_examples * 100"
      ],
      "metadata": {
        "id": "2Z8vn_VINA_q"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, (features, lengths, labels) in enumerate(train_loader):\n",
        "        features, lengths, labels = features.to(DEVICE), lengths.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = model(features, lengths)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        if not batch_idx % 50:\n",
        "            print(f'Epoch: {epoch+1}/{NUM_EPOCHS} | Batch {batch_idx}/{len(train_loader)} | Loss: {loss:.4f}')\n",
        "\n",
        "    train_acc = compute_accuracy(model, train_loader)\n",
        "    valid_acc = compute_accuracy(model, valid_loader)\n",
        "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} | Train Acc: {train_acc:.2f}% | Valid Acc: {valid_acc:.2f}%')\n",
        "\n",
        "print(f'Total Training Time: {(time.time() - start_time) / 60:.2f} min')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Muqrv97jMZjQ",
        "outputId": "ae3d9bd7-4ba5-49ca-a952-ae0973aa103e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-19d22725cd7e>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=vocab['<pad>'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15 | Batch 0/266 | Loss: 0.6951\n",
            "Epoch: 1/15 | Batch 50/266 | Loss: 0.6990\n",
            "Epoch: 1/15 | Batch 100/266 | Loss: 0.6143\n",
            "Epoch: 1/15 | Batch 150/266 | Loss: 0.6441\n",
            "Epoch: 1/15 | Batch 200/266 | Loss: 0.5251\n",
            "Epoch: 1/15 | Batch 250/266 | Loss: 0.3624\n",
            "Epoch: 1/15 | Train Acc: 88.01% | Valid Acc: 85.38%\n",
            "Epoch: 2/15 | Batch 0/266 | Loss: 0.3162\n",
            "Epoch: 2/15 | Batch 50/266 | Loss: 0.2701\n",
            "Epoch: 2/15 | Batch 100/266 | Loss: 0.2254\n",
            "Epoch: 2/15 | Batch 150/266 | Loss: 0.2122\n",
            "Epoch: 2/15 | Batch 200/266 | Loss: 0.2418\n",
            "Epoch: 2/15 | Batch 250/266 | Loss: 0.2200\n",
            "Epoch: 2/15 | Train Acc: 94.65% | Valid Acc: 89.15%\n",
            "Epoch: 3/15 | Batch 0/266 | Loss: 0.1947\n",
            "Epoch: 3/15 | Batch 50/266 | Loss: 0.1602\n",
            "Epoch: 3/15 | Batch 100/266 | Loss: 0.2621\n",
            "Epoch: 3/15 | Batch 150/266 | Loss: 0.1707\n",
            "Epoch: 3/15 | Batch 200/266 | Loss: 0.1627\n",
            "Epoch: 3/15 | Batch 250/266 | Loss: 0.1388\n",
            "Epoch: 3/15 | Train Acc: 97.41% | Valid Acc: 89.05%\n",
            "Epoch: 4/15 | Batch 0/266 | Loss: 0.0993\n",
            "Epoch: 4/15 | Batch 50/266 | Loss: 0.0708\n",
            "Epoch: 4/15 | Batch 100/266 | Loss: 0.0931\n",
            "Epoch: 4/15 | Batch 150/266 | Loss: 0.0775\n",
            "Epoch: 4/15 | Batch 200/266 | Loss: 0.0844\n",
            "Epoch: 4/15 | Batch 250/266 | Loss: 0.0931\n",
            "Epoch: 4/15 | Train Acc: 98.65% | Valid Acc: 89.10%\n",
            "Epoch: 5/15 | Batch 0/266 | Loss: 0.0421\n",
            "Epoch: 5/15 | Batch 50/266 | Loss: 0.0362\n",
            "Epoch: 5/15 | Batch 100/266 | Loss: 0.0947\n",
            "Epoch: 5/15 | Batch 150/266 | Loss: 0.0137\n",
            "Epoch: 5/15 | Batch 200/266 | Loss: 0.0312\n",
            "Epoch: 5/15 | Batch 250/266 | Loss: 0.0350\n",
            "Epoch: 5/15 | Train Acc: 99.40% | Valid Acc: 89.53%\n",
            "Epoch: 6/15 | Batch 0/266 | Loss: 0.0200\n",
            "Epoch: 6/15 | Batch 50/266 | Loss: 0.0225\n",
            "Epoch: 6/15 | Batch 100/266 | Loss: 0.0639\n",
            "Epoch: 6/15 | Batch 150/266 | Loss: 0.0566\n",
            "Epoch: 6/15 | Batch 200/266 | Loss: 0.0196\n",
            "Epoch: 6/15 | Batch 250/266 | Loss: 0.0167\n",
            "Epoch: 6/15 | Train Acc: 99.51% | Valid Acc: 89.25%\n",
            "Epoch: 7/15 | Batch 0/266 | Loss: 0.0113\n",
            "Epoch: 7/15 | Batch 50/266 | Loss: 0.0144\n",
            "Epoch: 7/15 | Batch 100/266 | Loss: 0.0025\n",
            "Epoch: 7/15 | Batch 150/266 | Loss: 0.0036\n",
            "Epoch: 7/15 | Batch 200/266 | Loss: 0.0062\n",
            "Epoch: 7/15 | Batch 250/266 | Loss: 0.0078\n",
            "Epoch: 7/15 | Train Acc: 99.71% | Valid Acc: 89.60%\n",
            "Epoch: 8/15 | Batch 0/266 | Loss: 0.0109\n",
            "Epoch: 8/15 | Batch 50/266 | Loss: 0.0382\n",
            "Epoch: 8/15 | Batch 100/266 | Loss: 0.0211\n",
            "Epoch: 8/15 | Batch 150/266 | Loss: 0.0132\n",
            "Epoch: 8/15 | Batch 200/266 | Loss: 0.0397\n",
            "Epoch: 8/15 | Batch 250/266 | Loss: 0.0266\n",
            "Epoch: 8/15 | Train Acc: 99.61% | Valid Acc: 89.55%\n",
            "Epoch: 9/15 | Batch 0/266 | Loss: 0.0042\n",
            "Epoch: 9/15 | Batch 50/266 | Loss: 0.0128\n",
            "Epoch: 9/15 | Batch 100/266 | Loss: 0.0309\n",
            "Epoch: 9/15 | Batch 150/266 | Loss: 0.0223\n",
            "Epoch: 9/15 | Batch 200/266 | Loss: 0.0195\n",
            "Epoch: 9/15 | Batch 250/266 | Loss: 0.0552\n",
            "Epoch: 9/15 | Train Acc: 99.24% | Valid Acc: 88.37%\n",
            "Epoch: 10/15 | Batch 0/266 | Loss: 0.0151\n",
            "Epoch: 10/15 | Batch 50/266 | Loss: 0.0158\n",
            "Epoch: 10/15 | Batch 100/266 | Loss: 0.0014\n",
            "Epoch: 10/15 | Batch 150/266 | Loss: 0.0349\n",
            "Epoch: 10/15 | Batch 200/266 | Loss: 0.0207\n",
            "Epoch: 10/15 | Batch 250/266 | Loss: 0.0429\n",
            "Epoch: 10/15 | Train Acc: 99.68% | Valid Acc: 89.43%\n",
            "Epoch: 11/15 | Batch 0/266 | Loss: 0.0037\n",
            "Epoch: 11/15 | Batch 50/266 | Loss: 0.0095\n",
            "Epoch: 11/15 | Batch 100/266 | Loss: 0.0095\n",
            "Epoch: 11/15 | Batch 150/266 | Loss: 0.0164\n",
            "Epoch: 11/15 | Batch 200/266 | Loss: 0.0049\n",
            "Epoch: 11/15 | Batch 250/266 | Loss: 0.0136\n",
            "Epoch: 11/15 | Train Acc: 99.84% | Valid Acc: 89.23%\n",
            "Epoch: 12/15 | Batch 0/266 | Loss: 0.0026\n",
            "Epoch: 12/15 | Batch 50/266 | Loss: 0.0122\n",
            "Epoch: 12/15 | Batch 100/266 | Loss: 0.0093\n",
            "Epoch: 12/15 | Batch 150/266 | Loss: 0.0063\n",
            "Epoch: 12/15 | Batch 200/266 | Loss: 0.0255\n",
            "Epoch: 12/15 | Batch 250/266 | Loss: 0.0154\n",
            "Epoch: 12/15 | Train Acc: 99.72% | Valid Acc: 88.92%\n",
            "Epoch: 13/15 | Batch 0/266 | Loss: 0.0010\n",
            "Epoch: 13/15 | Batch 50/266 | Loss: 0.0015\n",
            "Epoch: 13/15 | Batch 100/266 | Loss: 0.0549\n",
            "Epoch: 13/15 | Batch 150/266 | Loss: 0.0015\n",
            "Epoch: 13/15 | Batch 200/266 | Loss: 0.0051\n",
            "Epoch: 13/15 | Batch 250/266 | Loss: 0.0267\n",
            "Epoch: 13/15 | Train Acc: 99.67% | Valid Acc: 88.60%\n",
            "Epoch: 14/15 | Batch 0/266 | Loss: 0.0085\n",
            "Epoch: 14/15 | Batch 50/266 | Loss: 0.0007\n",
            "Epoch: 14/15 | Batch 100/266 | Loss: 0.0022\n",
            "Epoch: 14/15 | Batch 150/266 | Loss: 0.0125\n",
            "Epoch: 14/15 | Batch 200/266 | Loss: 0.0056\n",
            "Epoch: 14/15 | Batch 250/266 | Loss: 0.0065\n",
            "Epoch: 14/15 | Train Acc: 99.82% | Valid Acc: 89.38%\n",
            "Epoch: 15/15 | Batch 0/266 | Loss: 0.0008\n",
            "Epoch: 15/15 | Batch 50/266 | Loss: 0.0084\n",
            "Epoch: 15/15 | Batch 100/266 | Loss: 0.0099\n",
            "Epoch: 15/15 | Batch 150/266 | Loss: 0.0009\n",
            "Epoch: 15/15 | Batch 200/266 | Loss: 0.0092\n",
            "Epoch: 15/15 | Batch 250/266 | Loss: 0.0170\n",
            "Epoch: 15/15 | Train Acc: 99.84% | Valid Acc: 88.85%\n",
            "Total Training Time: 21.51 min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = compute_accuracy(model, test_loader)\n",
        "print(f'Test Acc: {test_acc:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flPlDo5HUJGy",
        "outputId": "eee0a119-54a6-449e-dc87-3220af1d5729"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-19d22725cd7e>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  texts = torch.nn.utils.rnn.pad_sequence([torch.tensor(text) for text in texts], batch_first=True, padding_value=vocab['<pad>'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Acc: 88.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.eval()\n",
        "    tokens = tokenizer(sentence)\n",
        "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "    length = torch.tensor([len(indices)])\n",
        "    tensor = torch.tensor(indices).unsqueeze(0).to(DEVICE)\n",
        "    length = length.to(DEVICE)\n",
        "    output = model(tensor, length)\n",
        "    prob = torch.nn.functional.softmax(output, dim=1)\n",
        "    return prob[0][1].item()\n",
        "\n",
        "# Example Predictions\n",
        "print(\"This is an awesome movie! ->\", \"Positive Sentiment probability:\", predict_sentiment(model, \"This is an awesome movie!\"))\n",
        "print(\"I hated this movie. ->\", \"Positive Sentiment probability:\", predict_sentiment(model, \"I hated this movie.\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOB-XUDmUJJQ",
        "outputId": "ef2eb522-2829-4d45-b050-17b6f574cf95"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an awesome movie! -> Positive Sentiment probability: 0.9999961853027344\n",
            "I hated this movie. -> Positive Sentiment probability: 1.108322635445802e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_tqsGDS7UJK3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}