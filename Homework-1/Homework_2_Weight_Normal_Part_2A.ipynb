{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pL7nZyn9lBsQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import sampler\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "def get_dataloaders_mnist(batch_size, num_workers=0,\n",
        "                          validation_fraction=None,\n",
        "                          train_transforms=None,\n",
        "                          test_transforms=None):\n",
        "\n",
        "    if train_transforms is None:\n",
        "        train_transforms = transforms.ToTensor()\n",
        "\n",
        "    if test_transforms is None:\n",
        "        test_transforms = transforms.ToTensor()\n",
        "\n",
        "    train_dataset = datasets.MNIST(root='data',\n",
        "                                   train=True,\n",
        "                                   transform=train_transforms,\n",
        "                                   download=True)\n",
        "\n",
        "    valid_dataset = datasets.MNIST(root='data',\n",
        "                                   train=True,\n",
        "                                   transform=test_transforms)\n",
        "\n",
        "    test_dataset = datasets.MNIST(root='data',\n",
        "                                  train=False,\n",
        "                                  transform=test_transforms)\n",
        "\n",
        "    if validation_fraction is not None:\n",
        "        num = int(validation_fraction * 60000)\n",
        "        train_indices = torch.arange(0, 60000 - num)\n",
        "        valid_indices = torch.arange(60000 - num, 60000)\n",
        "\n",
        "        train_sampler = SubsetRandomSampler(train_indices)\n",
        "        valid_sampler = SubsetRandomSampler(valid_indices)\n",
        "\n",
        "        valid_loader = DataLoader(dataset=valid_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  sampler=valid_sampler)\n",
        "\n",
        "        train_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  sampler=train_sampler)\n",
        "\n",
        "    else:\n",
        "        train_loader = DataLoader(dataset=train_dataset,\n",
        "                                  batch_size=batch_size,\n",
        "                                  num_workers=num_workers,\n",
        "                                  drop_last=True,\n",
        "                                  shuffle=True)\n",
        "\n",
        "    test_loader = DataLoader(dataset=test_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             num_workers=num_workers,\n",
        "                             shuffle=False)\n",
        "\n",
        "    if validation_fraction is None:\n",
        "        return train_loader, test_loader\n",
        "    else:\n",
        "        return train_loader, valid_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports from installed libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from distutils.version import LooseVersion as Version\n",
        "\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def set_deterministic():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    if torch.__version__ <= Version(\"1.7\"):\n",
        "        torch.set_deterministic(True)\n",
        "    else:\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.float().to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "ggoahREMlNKE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imports from installed libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from distutils.version import LooseVersion as Version\n",
        "\n",
        "\n",
        "def set_all_seeds(seed):\n",
        "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def set_deterministic():\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    if torch.__version__ <= Version(\"1.7\"):\n",
        "        torch.set_deterministic(True)\n",
        "    else:\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "\n",
        "\n",
        "def compute_accuracy(model, data_loader, device):\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        correct_pred, num_examples = 0, 0\n",
        "\n",
        "        for i, (features, targets) in enumerate(data_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.float().to(device)\n",
        "\n",
        "            logits = model(features)\n",
        "            _, predicted_labels = torch.max(logits, 1)\n",
        "\n",
        "            num_examples += targets.size(0)\n",
        "            correct_pred += (predicted_labels == targets).sum()\n",
        "    return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "4hxkq-KklSHY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_model(model, num_epochs, train_loader,\n",
        "                valid_loader, test_loader, optimizer, device):\n",
        "\n",
        "    start_time = time.time()\n",
        "    minibatch_loss_list, train_acc_list, valid_acc_list = [], [], []\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (features, targets) in enumerate(train_loader):\n",
        "\n",
        "            features = features.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # ## FORWARD AND BACK PROP\n",
        "            logits = model(features)\n",
        "            loss = torch.nn.functional.cross_entropy(logits, targets)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # ## UPDATE MODEL PARAMETERS\n",
        "            optimizer.step()\n",
        "\n",
        "            # ## LOGGING\n",
        "            minibatch_loss_list.append(loss.item())\n",
        "            if not batch_idx % 50:\n",
        "                print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                      f'| Batch {batch_idx:04d}/{len(train_loader):04d} '\n",
        "                      f'| Loss: {loss:.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # save memory during inference\n",
        "            train_acc = compute_accuracy(model, train_loader, device=device)\n",
        "            valid_acc = compute_accuracy(model, valid_loader, device=device)\n",
        "            print(f'Epoch: {epoch+1:03d}/{num_epochs:03d} '\n",
        "                  f'| Train: {train_acc :.2f}% '\n",
        "                  f'| Validation: {valid_acc :.2f}%')\n",
        "            train_acc_list.append(train_acc.item())\n",
        "            valid_acc_list.append(valid_acc.item())\n",
        "\n",
        "        elapsed = (time.time() - start_time)/60\n",
        "        print(f'Time elapsed: {elapsed:.2f} min')\n",
        "\n",
        "    elapsed = (time.time() - start_time)/60\n",
        "    print(f'Total Training Time: {elapsed:.2f} min')\n",
        "\n",
        "    test_acc = compute_accuracy(model, test_loader, device=device)\n",
        "    print(f'Test accuracy {test_acc :.2f}%')\n",
        "\n",
        "    return minibatch_loss_list, train_acc_list, valid_acc_list"
      ],
      "metadata": {
        "id": "d7l22ZUGlKpC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "CZOEOxgFlKlq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123\n",
        "BATCH_SIZE = 256\n",
        "NUM_HIDDEN_1 = 75\n",
        "NUM_HIDDEN_2 = 45\n",
        "NUM_EPOCHS = 50\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-iEx3mPClKjK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_all_seeds(RANDOM_SEED)\n"
      ],
      "metadata": {
        "id": "41UniRhOlKgx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = get_dataloaders_mnist(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    validation_fraction=0.1)\n",
        "\n",
        "# Checking the dataset\n",
        "for images, labels in train_loader:\n",
        "    print('Image batch dimensions:', images.shape)\n",
        "    print('Image label dimensions:', labels.shape)\n",
        "    print('Class labels of 10 examples:', labels[:10])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15n_76lBlKe6",
        "outputId": "da161f56-f376-4414-aaeb-5cb540e19a81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 505kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.55MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.42MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Image batch dimensions: torch.Size([256, 1, 28, 28])\n",
            "Image label dimensions: torch.Size([256])\n",
            "Class labels of 10 examples: tensor([4, 5, 8, 9, 9, 4, 9, 9, 3, 9])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilayerPerceptron(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_features, num_classes, drop_proba,\n",
        "                 num_hidden_1, num_hidden_2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.my_network = torch.nn.Sequential(\n",
        "            # 1st hidden layer\n",
        "            torch.nn.Flatten(),\n",
        "            torch.nn.Linear(num_features, num_hidden_1),\n",
        "            torch.nn.ReLU(),\n",
        "            # 2nd hidden layer\n",
        "            torch.nn.Linear(num_hidden_1, num_hidden_2),\n",
        "            torch.nn.ReLU(),\n",
        "            # output layer\n",
        "            torch.nn.Linear(num_hidden_2, num_classes)\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, torch.nn.Linear):\n",
        "                m.weight.detach().normal_(0, 0.001)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.detach().zero_()\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = self.my_network(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "PlYxZzjRlKbU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = MultilayerPerceptron(num_features=28*28,\n",
        "                             num_hidden_1=NUM_HIDDEN_1,\n",
        "                             num_hidden_2=NUM_HIDDEN_2,\n",
        "                             drop_proba=0.5,\n",
        "                             num_classes=10)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n"
      ],
      "metadata": {
        "id": "68Neh0l1lKYp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minibatch_loss_list, train_acc_list, valid_acc_list = train_model(\n",
        "    model=model,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    train_loader=train_loader,\n",
        "    valid_loader=valid_loader,\n",
        "    test_loader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    device=DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkyVElKulKWH",
        "outputId": "d1304e6b-37a7-46fd-ed6c-4a6ccff9de0b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/050 | Batch 0000/0210 | Loss: 2.3026\n",
            "Epoch: 001/050 | Batch 0050/0210 | Loss: 2.3003\n",
            "Epoch: 001/050 | Batch 0100/0210 | Loss: 2.3044\n",
            "Epoch: 001/050 | Batch 0150/0210 | Loss: 2.2988\n",
            "Epoch: 001/050 | Batch 0200/0210 | Loss: 2.2980\n",
            "Epoch: 001/050 | Train: 11.31% | Validation: 10.50%\n",
            "Time elapsed: 0.24 min\n",
            "Epoch: 002/050 | Batch 0000/0210 | Loss: 2.3005\n",
            "Epoch: 002/050 | Batch 0050/0210 | Loss: 2.2974\n",
            "Epoch: 002/050 | Batch 0100/0210 | Loss: 2.3039\n",
            "Epoch: 002/050 | Batch 0150/0210 | Loss: 2.3035\n",
            "Epoch: 002/050 | Batch 0200/0210 | Loss: 2.3043\n",
            "Epoch: 002/050 | Train: 11.31% | Validation: 10.50%\n",
            "Time elapsed: 0.46 min\n",
            "Epoch: 003/050 | Batch 0000/0210 | Loss: 2.3041\n",
            "Epoch: 003/050 | Batch 0050/0210 | Loss: 2.3008\n",
            "Epoch: 003/050 | Batch 0100/0210 | Loss: 2.3050\n",
            "Epoch: 003/050 | Batch 0150/0210 | Loss: 2.2976\n",
            "Epoch: 003/050 | Batch 0200/0210 | Loss: 2.2991\n",
            "Epoch: 003/050 | Train: 11.32% | Validation: 10.50%\n",
            "Time elapsed: 0.68 min\n",
            "Epoch: 004/050 | Batch 0000/0210 | Loss: 2.2990\n",
            "Epoch: 004/050 | Batch 0050/0210 | Loss: 2.2989\n",
            "Epoch: 004/050 | Batch 0100/0210 | Loss: 2.3014\n",
            "Epoch: 004/050 | Batch 0150/0210 | Loss: 2.3079\n",
            "Epoch: 004/050 | Batch 0200/0210 | Loss: 2.3031\n",
            "Epoch: 004/050 | Train: 11.33% | Validation: 10.50%\n",
            "Time elapsed: 0.90 min\n",
            "Epoch: 005/050 | Batch 0000/0210 | Loss: 2.3085\n",
            "Epoch: 005/050 | Batch 0050/0210 | Loss: 2.3038\n",
            "Epoch: 005/050 | Batch 0100/0210 | Loss: 2.2980\n",
            "Epoch: 005/050 | Batch 0150/0210 | Loss: 2.3036\n",
            "Epoch: 005/050 | Batch 0200/0210 | Loss: 2.3024\n",
            "Epoch: 005/050 | Train: 11.32% | Validation: 10.50%\n",
            "Time elapsed: 1.12 min\n",
            "Epoch: 006/050 | Batch 0000/0210 | Loss: 2.2992\n",
            "Epoch: 006/050 | Batch 0050/0210 | Loss: 2.3003\n",
            "Epoch: 006/050 | Batch 0100/0210 | Loss: 2.2955\n",
            "Epoch: 006/050 | Batch 0150/0210 | Loss: 2.3057\n",
            "Epoch: 006/050 | Batch 0200/0210 | Loss: 2.2965\n",
            "Epoch: 006/050 | Train: 11.33% | Validation: 10.50%\n",
            "Time elapsed: 1.34 min\n",
            "Epoch: 007/050 | Batch 0000/0210 | Loss: 2.3081\n",
            "Epoch: 007/050 | Batch 0050/0210 | Loss: 2.2975\n",
            "Epoch: 007/050 | Batch 0100/0210 | Loss: 2.2975\n",
            "Epoch: 007/050 | Batch 0150/0210 | Loss: 2.3064\n",
            "Epoch: 007/050 | Batch 0200/0210 | Loss: 2.2993\n",
            "Epoch: 007/050 | Train: 11.32% | Validation: 10.50%\n",
            "Time elapsed: 1.56 min\n",
            "Epoch: 008/050 | Batch 0000/0210 | Loss: 2.3036\n",
            "Epoch: 008/050 | Batch 0050/0210 | Loss: 2.3085\n",
            "Epoch: 008/050 | Batch 0100/0210 | Loss: 2.2990\n",
            "Epoch: 008/050 | Batch 0150/0210 | Loss: 2.2957\n",
            "Epoch: 008/050 | Batch 0200/0210 | Loss: 2.3017\n",
            "Epoch: 008/050 | Train: 11.32% | Validation: 10.50%\n",
            "Time elapsed: 1.78 min\n",
            "Epoch: 009/050 | Batch 0000/0210 | Loss: 2.3088\n",
            "Epoch: 009/050 | Batch 0050/0210 | Loss: 2.2996\n",
            "Epoch: 009/050 | Batch 0100/0210 | Loss: 2.3021\n",
            "Epoch: 009/050 | Batch 0150/0210 | Loss: 2.2992\n",
            "Epoch: 009/050 | Batch 0200/0210 | Loss: 2.3015\n",
            "Epoch: 009/050 | Train: 11.31% | Validation: 10.50%\n",
            "Time elapsed: 2.00 min\n",
            "Epoch: 010/050 | Batch 0000/0210 | Loss: 2.2991\n",
            "Epoch: 010/050 | Batch 0050/0210 | Loss: 2.3006\n",
            "Epoch: 010/050 | Batch 0100/0210 | Loss: 2.3097\n",
            "Epoch: 010/050 | Batch 0150/0210 | Loss: 2.3078\n",
            "Epoch: 010/050 | Batch 0200/0210 | Loss: 2.3047\n",
            "Epoch: 010/050 | Train: 11.32% | Validation: 10.50%\n",
            "Time elapsed: 2.21 min\n",
            "Epoch: 011/050 | Batch 0000/0210 | Loss: 2.2971\n",
            "Epoch: 011/050 | Batch 0050/0210 | Loss: 2.3076\n",
            "Epoch: 011/050 | Batch 0100/0210 | Loss: 2.3061\n",
            "Epoch: 011/050 | Batch 0150/0210 | Loss: 2.2986\n",
            "Epoch: 011/050 | Batch 0200/0210 | Loss: 2.2913\n",
            "Epoch: 011/050 | Train: 11.31% | Validation: 10.50%\n",
            "Time elapsed: 2.43 min\n",
            "Epoch: 012/050 | Batch 0000/0210 | Loss: 2.2981\n",
            "Epoch: 012/050 | Batch 0050/0210 | Loss: 2.2975\n",
            "Epoch: 012/050 | Batch 0100/0210 | Loss: 2.2965\n",
            "Epoch: 012/050 | Batch 0150/0210 | Loss: 2.3053\n",
            "Epoch: 012/050 | Batch 0200/0210 | Loss: 2.2965\n",
            "Epoch: 012/050 | Train: 11.30% | Validation: 10.50%\n",
            "Time elapsed: 2.64 min\n",
            "Epoch: 013/050 | Batch 0000/0210 | Loss: 2.2963\n",
            "Epoch: 013/050 | Batch 0050/0210 | Loss: 2.3000\n",
            "Epoch: 013/050 | Batch 0100/0210 | Loss: 2.2988\n",
            "Epoch: 013/050 | Batch 0150/0210 | Loss: 2.3032\n",
            "Epoch: 013/050 | Batch 0200/0210 | Loss: 2.3008\n",
            "Epoch: 013/050 | Train: 11.31% | Validation: 10.50%\n",
            "Time elapsed: 2.86 min\n",
            "Epoch: 014/050 | Batch 0000/0210 | Loss: 2.2977\n",
            "Epoch: 014/050 | Batch 0050/0210 | Loss: 2.3084\n",
            "Epoch: 014/050 | Batch 0100/0210 | Loss: 2.3002\n",
            "Epoch: 014/050 | Batch 0150/0210 | Loss: 2.3007\n",
            "Epoch: 014/050 | Batch 0200/0210 | Loss: 2.2999\n",
            "Epoch: 014/050 | Train: 11.31% | Validation: 10.50%\n",
            "Time elapsed: 3.08 min\n",
            "Epoch: 015/050 | Batch 0000/0210 | Loss: 2.2984\n",
            "Epoch: 015/050 | Batch 0050/0210 | Loss: 2.3014\n",
            "Epoch: 015/050 | Batch 0100/0210 | Loss: 2.3033\n",
            "Epoch: 015/050 | Batch 0150/0210 | Loss: 2.3003\n",
            "Epoch: 015/050 | Batch 0200/0210 | Loss: 2.2942\n",
            "Epoch: 015/050 | Train: 11.32% | Validation: 10.50%\n",
            "Time elapsed: 3.30 min\n",
            "Epoch: 016/050 | Batch 0000/0210 | Loss: 2.3000\n",
            "Epoch: 016/050 | Batch 0050/0210 | Loss: 2.2978\n",
            "Epoch: 016/050 | Batch 0100/0210 | Loss: 2.3044\n",
            "Epoch: 016/050 | Batch 0150/0210 | Loss: 2.3012\n",
            "Epoch: 016/050 | Batch 0200/0210 | Loss: 2.3065\n",
            "Epoch: 016/050 | Train: 11.30% | Validation: 10.50%\n",
            "Time elapsed: 3.51 min\n",
            "Epoch: 017/050 | Batch 0000/0210 | Loss: 2.2976\n",
            "Epoch: 017/050 | Batch 0050/0210 | Loss: 2.2968\n",
            "Epoch: 017/050 | Batch 0100/0210 | Loss: 2.3030\n",
            "Epoch: 017/050 | Batch 0150/0210 | Loss: 2.3046\n",
            "Epoch: 017/050 | Batch 0200/0210 | Loss: 2.2981\n",
            "Epoch: 017/050 | Train: 11.33% | Validation: 10.50%\n",
            "Time elapsed: 3.73 min\n",
            "Epoch: 018/050 | Batch 0000/0210 | Loss: 2.2999\n",
            "Epoch: 018/050 | Batch 0050/0210 | Loss: 2.2843\n",
            "Epoch: 018/050 | Batch 0100/0210 | Loss: 2.2289\n",
            "Epoch: 018/050 | Batch 0150/0210 | Loss: 2.0106\n",
            "Epoch: 018/050 | Batch 0200/0210 | Loss: 1.9151\n",
            "Epoch: 018/050 | Train: 26.08% | Validation: 26.13%\n",
            "Time elapsed: 3.98 min\n",
            "Epoch: 019/050 | Batch 0000/0210 | Loss: 1.8482\n",
            "Epoch: 019/050 | Batch 0050/0210 | Loss: 1.6901\n",
            "Epoch: 019/050 | Batch 0100/0210 | Loss: 1.6010\n",
            "Epoch: 019/050 | Batch 0150/0210 | Loss: 1.5598\n",
            "Epoch: 019/050 | Batch 0200/0210 | Loss: 1.4743\n",
            "Epoch: 019/050 | Train: 40.62% | Validation: 42.75%\n",
            "Time elapsed: 4.20 min\n",
            "Epoch: 020/050 | Batch 0000/0210 | Loss: 1.4900\n",
            "Epoch: 020/050 | Batch 0050/0210 | Loss: 1.3161\n",
            "Epoch: 020/050 | Batch 0100/0210 | Loss: 1.3590\n",
            "Epoch: 020/050 | Batch 0150/0210 | Loss: 1.3288\n",
            "Epoch: 020/050 | Batch 0200/0210 | Loss: 1.3530\n",
            "Epoch: 020/050 | Train: 46.05% | Validation: 47.47%\n",
            "Time elapsed: 4.42 min\n",
            "Epoch: 021/050 | Batch 0000/0210 | Loss: 1.3813\n",
            "Epoch: 021/050 | Batch 0050/0210 | Loss: 1.2991\n",
            "Epoch: 021/050 | Batch 0100/0210 | Loss: 1.2086\n",
            "Epoch: 021/050 | Batch 0150/0210 | Loss: 1.2774\n",
            "Epoch: 021/050 | Batch 0200/0210 | Loss: 1.1070\n",
            "Epoch: 021/050 | Train: 53.27% | Validation: 55.53%\n",
            "Time elapsed: 4.63 min\n",
            "Epoch: 022/050 | Batch 0000/0210 | Loss: 1.2741\n",
            "Epoch: 022/050 | Batch 0050/0210 | Loss: 1.0397\n",
            "Epoch: 022/050 | Batch 0100/0210 | Loss: 0.8508\n",
            "Epoch: 022/050 | Batch 0150/0210 | Loss: 0.6931\n",
            "Epoch: 022/050 | Batch 0200/0210 | Loss: 0.5985\n",
            "Epoch: 022/050 | Train: 77.08% | Validation: 80.18%\n",
            "Time elapsed: 4.85 min\n",
            "Epoch: 023/050 | Batch 0000/0210 | Loss: 0.6064\n",
            "Epoch: 023/050 | Batch 0050/0210 | Loss: 0.5445\n",
            "Epoch: 023/050 | Batch 0100/0210 | Loss: 0.5845\n",
            "Epoch: 023/050 | Batch 0150/0210 | Loss: 0.5426\n",
            "Epoch: 023/050 | Batch 0200/0210 | Loss: 0.5459\n",
            "Epoch: 023/050 | Train: 84.93% | Validation: 87.35%\n",
            "Time elapsed: 5.07 min\n",
            "Epoch: 024/050 | Batch 0000/0210 | Loss: 0.4241\n",
            "Epoch: 024/050 | Batch 0050/0210 | Loss: 0.5567\n",
            "Epoch: 024/050 | Batch 0100/0210 | Loss: 0.5268\n",
            "Epoch: 024/050 | Batch 0150/0210 | Loss: 0.4654\n",
            "Epoch: 024/050 | Batch 0200/0210 | Loss: 0.3704\n",
            "Epoch: 024/050 | Train: 87.21% | Validation: 89.52%\n",
            "Time elapsed: 5.29 min\n",
            "Epoch: 025/050 | Batch 0000/0210 | Loss: 0.4827\n",
            "Epoch: 025/050 | Batch 0050/0210 | Loss: 0.3438\n",
            "Epoch: 025/050 | Batch 0100/0210 | Loss: 0.3199\n",
            "Epoch: 025/050 | Batch 0150/0210 | Loss: 0.4290\n",
            "Epoch: 025/050 | Batch 0200/0210 | Loss: 0.4218\n",
            "Epoch: 025/050 | Train: 88.98% | Validation: 90.57%\n",
            "Time elapsed: 5.51 min\n",
            "Epoch: 026/050 | Batch 0000/0210 | Loss: 0.3564\n",
            "Epoch: 026/050 | Batch 0050/0210 | Loss: 0.4120\n",
            "Epoch: 026/050 | Batch 0100/0210 | Loss: 0.4037\n",
            "Epoch: 026/050 | Batch 0150/0210 | Loss: 0.3665\n",
            "Epoch: 026/050 | Batch 0200/0210 | Loss: 0.3301\n",
            "Epoch: 026/050 | Train: 90.94% | Validation: 92.55%\n",
            "Time elapsed: 5.73 min\n",
            "Epoch: 027/050 | Batch 0000/0210 | Loss: 0.3190\n",
            "Epoch: 027/050 | Batch 0050/0210 | Loss: 0.2743\n",
            "Epoch: 027/050 | Batch 0100/0210 | Loss: 0.3128\n",
            "Epoch: 027/050 | Batch 0150/0210 | Loss: 0.2725\n",
            "Epoch: 027/050 | Batch 0200/0210 | Loss: 0.3409\n",
            "Epoch: 027/050 | Train: 92.21% | Validation: 93.42%\n",
            "Time elapsed: 5.95 min\n",
            "Epoch: 028/050 | Batch 0000/0210 | Loss: 0.1905\n",
            "Epoch: 028/050 | Batch 0050/0210 | Loss: 0.2813\n",
            "Epoch: 028/050 | Batch 0100/0210 | Loss: 0.2290\n",
            "Epoch: 028/050 | Batch 0150/0210 | Loss: 0.1745\n",
            "Epoch: 028/050 | Batch 0200/0210 | Loss: 0.2777\n",
            "Epoch: 028/050 | Train: 92.57% | Validation: 94.02%\n",
            "Time elapsed: 6.17 min\n",
            "Epoch: 029/050 | Batch 0000/0210 | Loss: 0.2779\n",
            "Epoch: 029/050 | Batch 0050/0210 | Loss: 0.2028\n",
            "Epoch: 029/050 | Batch 0100/0210 | Loss: 0.2937\n",
            "Epoch: 029/050 | Batch 0150/0210 | Loss: 0.2761\n",
            "Epoch: 029/050 | Batch 0200/0210 | Loss: 0.1809\n",
            "Epoch: 029/050 | Train: 93.85% | Validation: 94.95%\n",
            "Time elapsed: 6.39 min\n",
            "Epoch: 030/050 | Batch 0000/0210 | Loss: 0.1894\n",
            "Epoch: 030/050 | Batch 0050/0210 | Loss: 0.2595\n",
            "Epoch: 030/050 | Batch 0100/0210 | Loss: 0.2702\n",
            "Epoch: 030/050 | Batch 0150/0210 | Loss: 0.1976\n",
            "Epoch: 030/050 | Batch 0200/0210 | Loss: 0.2528\n",
            "Epoch: 030/050 | Train: 94.22% | Validation: 94.95%\n",
            "Time elapsed: 6.62 min\n",
            "Epoch: 031/050 | Batch 0000/0210 | Loss: 0.2959\n",
            "Epoch: 031/050 | Batch 0050/0210 | Loss: 0.2479\n",
            "Epoch: 031/050 | Batch 0100/0210 | Loss: 0.2071\n",
            "Epoch: 031/050 | Batch 0150/0210 | Loss: 0.1888\n",
            "Epoch: 031/050 | Batch 0200/0210 | Loss: 0.2176\n",
            "Epoch: 031/050 | Train: 94.82% | Validation: 95.27%\n",
            "Time elapsed: 6.84 min\n",
            "Epoch: 032/050 | Batch 0000/0210 | Loss: 0.1558\n",
            "Epoch: 032/050 | Batch 0050/0210 | Loss: 0.1448\n",
            "Epoch: 032/050 | Batch 0100/0210 | Loss: 0.1474\n",
            "Epoch: 032/050 | Batch 0150/0210 | Loss: 0.1563\n",
            "Epoch: 032/050 | Batch 0200/0210 | Loss: 0.1437\n",
            "Epoch: 032/050 | Train: 95.00% | Validation: 95.68%\n",
            "Time elapsed: 7.05 min\n",
            "Epoch: 033/050 | Batch 0000/0210 | Loss: 0.1575\n",
            "Epoch: 033/050 | Batch 0050/0210 | Loss: 0.3308\n",
            "Epoch: 033/050 | Batch 0100/0210 | Loss: 0.1913\n",
            "Epoch: 033/050 | Batch 0150/0210 | Loss: 0.1577\n",
            "Epoch: 033/050 | Batch 0200/0210 | Loss: 0.1476\n",
            "Epoch: 033/050 | Train: 95.55% | Validation: 95.83%\n",
            "Time elapsed: 7.27 min\n",
            "Epoch: 034/050 | Batch 0000/0210 | Loss: 0.1265\n",
            "Epoch: 034/050 | Batch 0050/0210 | Loss: 0.0846\n",
            "Epoch: 034/050 | Batch 0100/0210 | Loss: 0.2111\n",
            "Epoch: 034/050 | Batch 0150/0210 | Loss: 0.2020\n",
            "Epoch: 034/050 | Batch 0200/0210 | Loss: 0.2131\n",
            "Epoch: 034/050 | Train: 95.94% | Validation: 96.02%\n",
            "Time elapsed: 7.50 min\n",
            "Epoch: 035/050 | Batch 0000/0210 | Loss: 0.1612\n",
            "Epoch: 035/050 | Batch 0050/0210 | Loss: 0.1365\n",
            "Epoch: 035/050 | Batch 0100/0210 | Loss: 0.1183\n",
            "Epoch: 035/050 | Batch 0150/0210 | Loss: 0.1410\n",
            "Epoch: 035/050 | Batch 0200/0210 | Loss: 0.1237\n",
            "Epoch: 035/050 | Train: 96.09% | Validation: 96.20%\n",
            "Time elapsed: 7.72 min\n",
            "Epoch: 036/050 | Batch 0000/0210 | Loss: 0.1506\n",
            "Epoch: 036/050 | Batch 0050/0210 | Loss: 0.1705\n",
            "Epoch: 036/050 | Batch 0100/0210 | Loss: 0.1410\n",
            "Epoch: 036/050 | Batch 0150/0210 | Loss: 0.1561\n",
            "Epoch: 036/050 | Batch 0200/0210 | Loss: 0.1631\n",
            "Epoch: 036/050 | Train: 96.34% | Validation: 96.17%\n",
            "Time elapsed: 7.93 min\n",
            "Epoch: 037/050 | Batch 0000/0210 | Loss: 0.1666\n",
            "Epoch: 037/050 | Batch 0050/0210 | Loss: 0.1096\n",
            "Epoch: 037/050 | Batch 0100/0210 | Loss: 0.0857\n",
            "Epoch: 037/050 | Batch 0150/0210 | Loss: 0.1716\n",
            "Epoch: 037/050 | Batch 0200/0210 | Loss: 0.1819\n",
            "Epoch: 037/050 | Train: 96.46% | Validation: 96.33%\n",
            "Time elapsed: 8.15 min\n",
            "Epoch: 038/050 | Batch 0000/0210 | Loss: 0.0784\n",
            "Epoch: 038/050 | Batch 0050/0210 | Loss: 0.0880\n",
            "Epoch: 038/050 | Batch 0100/0210 | Loss: 0.1118\n",
            "Epoch: 038/050 | Batch 0150/0210 | Loss: 0.1070\n",
            "Epoch: 038/050 | Batch 0200/0210 | Loss: 0.1541\n",
            "Epoch: 038/050 | Train: 96.85% | Validation: 96.72%\n",
            "Time elapsed: 8.37 min\n",
            "Epoch: 039/050 | Batch 0000/0210 | Loss: 0.0820\n",
            "Epoch: 039/050 | Batch 0050/0210 | Loss: 0.1022\n",
            "Epoch: 039/050 | Batch 0100/0210 | Loss: 0.0945\n",
            "Epoch: 039/050 | Batch 0150/0210 | Loss: 0.1169\n",
            "Epoch: 039/050 | Batch 0200/0210 | Loss: 0.1245\n",
            "Epoch: 039/050 | Train: 96.91% | Validation: 96.70%\n",
            "Time elapsed: 8.58 min\n",
            "Epoch: 040/050 | Batch 0000/0210 | Loss: 0.1024\n",
            "Epoch: 040/050 | Batch 0050/0210 | Loss: 0.0757\n",
            "Epoch: 040/050 | Batch 0100/0210 | Loss: 0.0963\n",
            "Epoch: 040/050 | Batch 0150/0210 | Loss: 0.0766\n",
            "Epoch: 040/050 | Batch 0200/0210 | Loss: 0.0893\n",
            "Epoch: 040/050 | Train: 97.22% | Validation: 96.85%\n",
            "Time elapsed: 8.80 min\n",
            "Epoch: 041/050 | Batch 0000/0210 | Loss: 0.0546\n",
            "Epoch: 041/050 | Batch 0050/0210 | Loss: 0.0934\n",
            "Epoch: 041/050 | Batch 0100/0210 | Loss: 0.0984\n",
            "Epoch: 041/050 | Batch 0150/0210 | Loss: 0.1601\n",
            "Epoch: 041/050 | Batch 0200/0210 | Loss: 0.1590\n",
            "Epoch: 041/050 | Train: 97.21% | Validation: 96.50%\n",
            "Time elapsed: 9.01 min\n",
            "Epoch: 042/050 | Batch 0000/0210 | Loss: 0.1276\n",
            "Epoch: 042/050 | Batch 0050/0210 | Loss: 0.0755\n",
            "Epoch: 042/050 | Batch 0100/0210 | Loss: 0.1214\n",
            "Epoch: 042/050 | Batch 0150/0210 | Loss: 0.1253\n",
            "Epoch: 042/050 | Batch 0200/0210 | Loss: 0.1025\n",
            "Epoch: 042/050 | Train: 97.53% | Validation: 96.95%\n",
            "Time elapsed: 9.23 min\n",
            "Epoch: 043/050 | Batch 0000/0210 | Loss: 0.0957\n",
            "Epoch: 043/050 | Batch 0050/0210 | Loss: 0.0765\n",
            "Epoch: 043/050 | Batch 0100/0210 | Loss: 0.0842\n",
            "Epoch: 043/050 | Batch 0150/0210 | Loss: 0.0906\n",
            "Epoch: 043/050 | Batch 0200/0210 | Loss: 0.1098\n",
            "Epoch: 043/050 | Train: 97.69% | Validation: 97.03%\n",
            "Time elapsed: 9.45 min\n",
            "Epoch: 044/050 | Batch 0000/0210 | Loss: 0.1053\n",
            "Epoch: 044/050 | Batch 0050/0210 | Loss: 0.0775\n",
            "Epoch: 044/050 | Batch 0100/0210 | Loss: 0.0787\n",
            "Epoch: 044/050 | Batch 0150/0210 | Loss: 0.0742\n",
            "Epoch: 044/050 | Batch 0200/0210 | Loss: 0.1085\n",
            "Epoch: 044/050 | Train: 97.77% | Validation: 97.20%\n",
            "Time elapsed: 9.67 min\n",
            "Epoch: 045/050 | Batch 0000/0210 | Loss: 0.0744\n",
            "Epoch: 045/050 | Batch 0050/0210 | Loss: 0.0506\n",
            "Epoch: 045/050 | Batch 0100/0210 | Loss: 0.0588\n",
            "Epoch: 045/050 | Batch 0150/0210 | Loss: 0.1035\n",
            "Epoch: 045/050 | Batch 0200/0210 | Loss: 0.0453\n",
            "Epoch: 045/050 | Train: 97.93% | Validation: 96.80%\n",
            "Time elapsed: 9.89 min\n",
            "Epoch: 046/050 | Batch 0000/0210 | Loss: 0.1285\n",
            "Epoch: 046/050 | Batch 0050/0210 | Loss: 0.0668\n",
            "Epoch: 046/050 | Batch 0100/0210 | Loss: 0.0486\n",
            "Epoch: 046/050 | Batch 0150/0210 | Loss: 0.1120\n",
            "Epoch: 046/050 | Batch 0200/0210 | Loss: 0.1360\n",
            "Epoch: 046/050 | Train: 98.03% | Validation: 97.13%\n",
            "Time elapsed: 10.11 min\n",
            "Epoch: 047/050 | Batch 0000/0210 | Loss: 0.0828\n",
            "Epoch: 047/050 | Batch 0050/0210 | Loss: 0.0512\n",
            "Epoch: 047/050 | Batch 0100/0210 | Loss: 0.0941\n",
            "Epoch: 047/050 | Batch 0150/0210 | Loss: 0.0531\n",
            "Epoch: 047/050 | Batch 0200/0210 | Loss: 0.0396\n",
            "Epoch: 047/050 | Train: 97.88% | Validation: 96.75%\n",
            "Time elapsed: 10.33 min\n",
            "Epoch: 048/050 | Batch 0000/0210 | Loss: 0.0626\n",
            "Epoch: 048/050 | Batch 0050/0210 | Loss: 0.0733\n",
            "Epoch: 048/050 | Batch 0100/0210 | Loss: 0.0346\n",
            "Epoch: 048/050 | Batch 0150/0210 | Loss: 0.0950\n",
            "Epoch: 048/050 | Batch 0200/0210 | Loss: 0.1240\n",
            "Epoch: 048/050 | Train: 98.14% | Validation: 96.88%\n",
            "Time elapsed: 10.54 min\n",
            "Epoch: 049/050 | Batch 0000/0210 | Loss: 0.0548\n",
            "Epoch: 049/050 | Batch 0050/0210 | Loss: 0.0590\n",
            "Epoch: 049/050 | Batch 0100/0210 | Loss: 0.0503\n",
            "Epoch: 049/050 | Batch 0150/0210 | Loss: 0.0619\n",
            "Epoch: 049/050 | Batch 0200/0210 | Loss: 0.0609\n",
            "Epoch: 049/050 | Train: 98.28% | Validation: 97.25%\n",
            "Time elapsed: 10.76 min\n",
            "Epoch: 050/050 | Batch 0000/0210 | Loss: 0.1310\n",
            "Epoch: 050/050 | Batch 0050/0210 | Loss: 0.0657\n",
            "Epoch: 050/050 | Batch 0100/0210 | Loss: 0.0321\n",
            "Epoch: 050/050 | Batch 0150/0210 | Loss: 0.0437\n",
            "Epoch: 050/050 | Batch 0200/0210 | Loss: 0.1500\n",
            "Epoch: 050/050 | Train: 98.30% | Validation: 97.25%\n",
            "Time elapsed: 10.98 min\n",
            "Total Training Time: 10.98 min\n",
            "Test accuracy 96.73%\n"
          ]
        }
      ]
    }
  ]
}